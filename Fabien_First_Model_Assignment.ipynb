{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FabienMiguel/NLP-Fellowship/blob/main/Fabien_First_Model_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTCDIKHKLJiu"
      },
      "source": [
        "First Model Assignment\n",
        "\n",
        "The aim of this assignment is to make sure you understand the foundations of model training. We have covered traditional ML and also simple NN. You task is\n",
        "\n",
        "to write code for training traditional ML model that gives the highest accuracy\n",
        "Code for NN model that gives the highest accuracy\n",
        "This we will consider\n",
        "The code works\n",
        "The hyperparameter used for fine tuning - epoch only is not enough\n",
        "The highest accuracy\n",
        "Bonus points for being creative with preprocessing, tokenization and creation of vectors\n",
        "Submissions\n",
        "Notebook with code\n",
        "2 models\n",
        "Deadline\n",
        "Monday 28th at 5pm\n",
        "\n",
        "ASSIGNMENT ANSWERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyaDOwCXaQXz",
        "outputId": "3d68b883-b47a-4b33-befb-77daa02d5737"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "mULIOo3z63iT",
        "outputId": "07b85f23-2eb7-4afc-88cd-51114db67f50"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  review  sentiment    set\n",
              "0      I went and saw this movie last night after bei...          1   test\n",
              "1      Actor turned director Bill Paxton follows up h...          1   test\n",
              "2      As a recreational golfer with some knowledge o...          1   test\n",
              "3      I saw this film in a sneak preview, and it is ...          1   test\n",
              "4      Bill Paxton has taken the true story of the 19...          1   test\n",
              "...                                                  ...        ...    ...\n",
              "49995  Towards the end of the movie, I felt it was to...          0  train\n",
              "49996  This is the kind of movie that my enemies cont...          0  train\n",
              "49997  I saw 'Descent' last night at the Stockholm Fi...          0  train\n",
              "49998  Some films that you pick up for a pound turn o...          0  train\n",
              "49999  This is one of the dumbest films, I've ever se...          0  train\n",
              "\n",
              "[50000 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-96cd3819-551a-4f3a-9f2b-0672328b6869\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>set</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I went and saw this movie last night after bei...</td>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>As a recreational golfer with some knowledge o...</td>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>Towards the end of the movie, I felt it was to...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>This is the kind of movie that my enemies cont...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>I saw 'Descent' last night at the Stockholm Fi...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>Some films that you pick up for a pound turn o...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>This is one of the dumbest films, I've ever se...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows Ã— 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-96cd3819-551a-4f3a-9f2b-0672328b6869')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-96cd3819-551a-4f3a-9f2b-0672328b6869 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-96cd3819-551a-4f3a-9f2b-0672328b6869');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "full_dataset = pd.read_csv('/content/drive/MyDrive/NLP Fellowship/50k_imdb_movie_reviews (1).csv')\n",
        "full_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anzIu32X7wdz",
        "outputId": "464e0d2d-28b6-4822-fd20-67b2eb1896e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.8.0\n",
            "  Downloading torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 735.5 MB 14 kB/s \n",
            "\u001b[?25hCollecting torchtext==0.9.0\n",
            "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.1 MB 17.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (4.64.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (1.24.3)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.13.1\n",
            "    Uninstalling torchtext-0.13.1:\n",
            "      Successfully uninstalled torchtext-0.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.8.0 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.8.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.0 torchtext-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.8.0 torchtext==0.9.0 #compatibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9Ac_Z-cF8exZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy.data import Dataset, Example\n",
        "from torchtext.legacy.data import BucketIterator\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMikfP398kgt",
        "outputId": "5daf589e-ad86-431b-c6b7-ec170ff47052"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count                                                 50000\n",
              "unique                                                49582\n",
              "top       Loved today's show!!! It was a variety and not...\n",
              "freq                                                      5\n",
              "Name: review, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "full_dataset['review'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0hZFlPHA-oTG"
      },
      "outputs": [],
      "source": [
        "full_dataset = full_dataset.drop_duplicates(subset=['review'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vp3Fa7sQ9Et4"
      },
      "outputs": [],
      "source": [
        "full_dataset = full_dataset.drop_duplicates(subset=['review'])\n",
        "train_dataset = full_dataset[(full_dataset['set'] == 'train')][['review','sentiment']]\n",
        "test_dataset = full_dataset[(full_dataset['set'] == 'test')][['review','sentiment']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VJyyQjCK-QbS"
      },
      "outputs": [],
      "source": [
        "# full_dataset\n",
        "def preprocessing(texts):\n",
        "  cleaned_text = []\n",
        "  for text in texts:\n",
        "    text = text.lower()\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                                u\"\\U00002702-\\U000027B0\"\n",
        "                                u\"\\U000024C2-\\U0001F251\"\n",
        "                                \"]+\", flags=re.UNICODE)\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    html_pattern = re.compile('<.*?>')\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "    text = url_pattern.sub(r'', text)\n",
        "    text = html_pattern.sub(r'', text)\n",
        "    text = re.sub(r\"[^\\w\\d'\\s]+\", ' ', text)\n",
        "    cleaned_text.append(text)\n",
        "\n",
        "  return cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lpb3R3bw-8JI",
        "outputId": "845b7e15-7a46-46da-f5c7-74dd1a03f608"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
          ]
        }
      ],
      "source": [
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True # Check this\n",
        "max_document_length = 300 #hyperparameter\n",
        "\n",
        "TEXT = data.Field(lower=True, include_lengths=True,  tokenize='spacy',preprocessing=preprocessing,batch_first=True,  fix_length=max_document_length)\n",
        "LABEL = data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "class DataFrameDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, fields: list):\n",
        "        super(DataFrameDataset, self).__init__(\n",
        "            [\n",
        "                Example.fromlist(list(r), fields) \n",
        "                for i, r in df.iterrows()\n",
        "            ], \n",
        "            fields\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rBTbU3GE_vZo"
      },
      "outputs": [],
      "source": [
        "\n",
        "torch_valid_dataset, torch_test_dataset = DataFrameDataset(\n",
        "    df=test_dataset, \n",
        "    fields=(\n",
        "        ('review', TEXT),\n",
        "        ('sentiment', LABEL)\n",
        "    )\n",
        ").split() \n",
        "\n",
        "torch_train_dataset = DataFrameDataset(\n",
        "    df=train_dataset, \n",
        "    fields=(\n",
        "        ('review', TEXT),\n",
        "        ('sentiment', LABEL)\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeJDuZNf_3LW",
        "outputId": "46346a9e-e7fe-465a-b66c-9095ec08ff58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/wiki.simple.vec: 293MB [00:35, 8.35MB/s]                           \n",
            "  0%|          | 0/111051 [00:00<?, ?it/s]WARNING:torchtext.vocab:Skipping token b'111051' with 1-dimensional vector [b'300']; likely a header\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111051/111051 [00:08<00:00, 12638.13it/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "max_size = 30000 #hyperparameter\n",
        "TEXT.build_vocab(torch_train_dataset, max_size=max_size,vectors='fasttext.simple.300d')\n",
        "vocab_size = len(TEXT.vocab)\n",
        "\n",
        "BATCH_SIZE = 64 #hyperparameter\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (torch_train_dataset, torch_valid_dataset, torch_test_dataset), \n",
        "    batch_size = BATCH_SIZE ,\n",
        "    sort_key=lambda x: len(x.review),\n",
        "    sort_within_batch=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fODaiwYlrkk_"
      },
      "source": [
        "**accuracy function to be accessed by both models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUb_KYVPCSeB"
      },
      "source": [
        "GLOBAL HYPERPARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "41SCgssgCXL1"
      },
      "outputs": [],
      "source": [
        "lr = 1e-3\n",
        "batch_size = 64\n",
        "dropout_keep_prob = 0.3\n",
        "embedding_size = 300\n",
        "max_document_length = 300 # each sentence has until 100 words\n",
        "vocab_size = len(TEXT.vocab)\n",
        "dev_size = 0.8 # split percentage to train\\validation data\n",
        "max_size = 30000 # maximum vocabulary size\n",
        "seed = 30\n",
        "num_classes = 2\n",
        "\n",
        "num_epochs = 15\n",
        "hidden_size = 256\n",
        "hidden_size1 = 300\n",
        "hidden_size2 = 128\n",
        "hidden_size3 = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lAqq4Nv0AHA9"
      },
      "outputs": [],
      "source": [
        "def accuracy(probs, target):\n",
        "  winners = probs.argmax(dim=1)\n",
        "  corrects = (winners == target)\n",
        "  accuracy = corrects.sum().float() / float(target.size(0))\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-H8WgeZsURp"
      },
      "source": [
        "**LR neural networks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "b1Ymng6wAH7Y"
      },
      "outputs": [],
      "source": [
        "# full_dataset\n",
        "class LR(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size,hidden_size2,hidden_size3, num_classes):\n",
        "        super(LR, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size) # \n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size2)\n",
        "        self.fc3 = nn.Linear(hidden_size2, hidden_size3) \n",
        "        self.fc4 = nn.Linear(hidden_size3, num_classes)\n",
        "\n",
        "    def forward(self, text):\n",
        "        text = text.float() # dense layer deals just with float type data\n",
        "        x = self.fc1(text) #(m x n) with (n x p)\n",
        "        x = self.relu(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        \n",
        "        preds = self.fc4(x) # crossentropyloss handles the softmax\n",
        "        # preds = F.softmax(preds,1) # nn.softmax\n",
        "        return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "LC1cJemqAHT-"
      },
      "outputs": [],
      "source": [
        "to_train = True\n",
        "LRmodel = LR(max_document_length, hidden_size,hidden_size2,hidden_size3, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bduRH2qAGuL",
        "outputId": "a1b72492-ffcd-4122-f98f-48e7e08b55bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 2.886 | Train Acc: 49.58%\n",
            "\t Val. Loss: 0.697 |  Val. Acc: 50.48%\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.25%\n",
            "\t Val. Loss: 0.699 |  Val. Acc: 50.05%\n",
            "\tTrain Loss: 0.692 | Train Acc: 51.08%\n",
            "\t Val. Loss: 0.697 |  Val. Acc: 50.23%\n",
            "\tTrain Loss: 0.689 | Train Acc: 50.88%\n",
            "\t Val. Loss: 0.695 |  Val. Acc: 50.39%\n",
            "\tTrain Loss: 0.687 | Train Acc: 52.03%\n",
            "\t Val. Loss: 0.705 |  Val. Acc: 50.07%\n",
            "\tTrain Loss: 0.685 | Train Acc: 52.01%\n",
            "\t Val. Loss: 0.696 |  Val. Acc: 50.35%\n",
            "\tTrain Loss: 0.683 | Train Acc: 52.32%\n",
            "\t Val. Loss: 0.699 |  Val. Acc: 50.42%\n",
            "\tTrain Loss: 0.685 | Train Acc: 52.05%\n",
            "\t Val. Loss: 0.703 |  Val. Acc: 50.47%\n",
            "\tTrain Loss: 0.685 | Train Acc: 52.04%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 50.07%\n",
            "\tTrain Loss: 0.682 | Train Acc: 52.04%\n",
            "\t Val. Loss: 0.705 |  Val. Acc: 50.54%\n",
            "\tTrain Loss: 0.685 | Train Acc: 52.18%\n",
            "\t Val. Loss: 0.703 |  Val. Acc: 50.06%\n",
            "\tTrain Loss: 0.684 | Train Acc: 52.39%\n",
            "\t Val. Loss: 0.718 |  Val. Acc: 50.25%\n",
            "\tTrain Loss: 0.682 | Train Acc: 53.03%\n",
            "\t Val. Loss: 0.731 |  Val. Acc: 50.43%\n",
            "\tTrain Loss: 0.683 | Train Acc: 52.12%\n",
            "\t Val. Loss: 0.740 |  Val. Acc: 50.09%\n",
            "\tTrain Loss: 0.686 | Train Acc: 52.24%\n",
            "\t Val. Loss: 0.728 |  Val. Acc: 50.17%\n"
          ]
        }
      ],
      "source": [
        "LRbest_valid_loss = float('inf')\n",
        "LRoptimizer = torch.optim.Adam(LRmodel.parameters(), lr=lr)\n",
        "\n",
        "LRloss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  LRtrain_epoch_loss = 0\n",
        "  LRtrain_epoch_acc = 0\n",
        "  for batch in train_iterator:\n",
        "      LRoptimizer.zero_grad()\n",
        "      # retrieve text and no. of words\n",
        "      text, text_lengths = batch.review\n",
        "\n",
        "      #feedforward\n",
        "      # model.to(device)\n",
        "      LRpredictions = LRmodel(text).squeeze(1)\n",
        "      \n",
        "      \n",
        "      loss = LRloss_func(LRpredictions, batch.sentiment)\n",
        "\n",
        "      acc = accuracy(LRpredictions, batch.sentiment)\n",
        "\n",
        "      # perform backpropagation\n",
        "      loss.backward()\n",
        "\n",
        "      LRoptimizer.step()\n",
        "\n",
        "      LRtrain_epoch_loss += loss.item()\n",
        "      LRtrain_epoch_acc += acc.item()\n",
        "\n",
        "  \n",
        "\n",
        "  LRvalid_epoch_loss = 0\n",
        "  LRvalid_epoch_acc = 0\n",
        "\n",
        "  LRmodel.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for batch in valid_iterator:\n",
        "          text, text_lengths = batch.review\n",
        "\n",
        "          LRpredictions = LRmodel(text).squeeze(1)\n",
        "\n",
        "          loss = LRloss_func(LRpredictions, batch.sentiment)\n",
        "\n",
        "          acc = accuracy(LRpredictions, batch.sentiment)\n",
        "\n",
        "          LRvalid_epoch_loss += loss.item()\n",
        "          LRvalid_epoch_acc += acc.item()\n",
        "\n",
        "   \n",
        "\n",
        "  if LRvalid_epoch_loss < LRbest_valid_loss:\n",
        "            LRbest_valid_loss = LRvalid_epoch_loss\n",
        "            torch.save(LRmodel.state_dict(), 'LRsaved_weights'+'_LRlinear.pt')\n",
        "\n",
        "  print(f'\\tTrain Loss: {LRtrain_epoch_loss / len(train_iterator):.3f} | Train Acc: {LRtrain_epoch_acc  / len(train_iterator)* 100:.2f}%')\n",
        "  print(f'\\t Val. Loss: {LRvalid_epoch_loss / len(valid_iterator):.3f} |  Val. Acc: {LRvalid_epoch_acc / len(valid_iterator) * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLQk8XDOr-sC"
      },
      "source": [
        "%%%%%%%%%%%%%%%%%%%%%%% multilayer percetron neural networks %%%%%%%%%%%%%%%%%%%%%%% "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4_ur0xeRAHq1"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size2, hidden_size3, hidden_size4, output_dim, dropout, max_document_length):\n",
        "        super().__init__()\n",
        "        # embedding and convolution layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc1 = nn.Linear(embed_size*max_document_length, hidden_size2)  # dense layer\n",
        "        self.fc2 = nn.Linear(hidden_size2, hidden_size3)  # dense layer\n",
        "        self.fc3 = nn.Linear(hidden_size3, hidden_size4)  # dense layer\n",
        "        self.fc4 = nn.Linear(hidden_size4, output_dim)  # dense layer\n",
        "\n",
        "    def forward(self, text):\n",
        "         # text shape = (batch_size, num_sequences)\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded = [batch size, sent_len, emb dim]\n",
        "        \n",
        "        x = embedded.view(embedded.shape[0], -1)  # x = Flatten()(x)\n",
        "        #embedded = embedded.unsqueeze(1) # fc gets 4 dimension\n",
        "        \n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.dropout(x)\n",
        "        preds = self.fc4(x)\n",
        "        # preds = F.softmax(preds, 1)\n",
        "        return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "iQzSCDVyAHfT"
      },
      "outputs": [],
      "source": [
        "\n",
        "to_train = True\n",
        "# model = LR(max_document_length, hidden_size,hidden_size2,hidden_size3, num_classes)\n",
        "model = MLP(vocab_size, embedding_size, hidden_size1, hidden_size2, hidden_size3,  num_classes, dropout_keep_prob, max_document_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dxj9ya13AGXv",
        "outputId": "d1b90439-213f-4805-f5c8-9264a4fe1704"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 0.763 | Train Acc: 50.19%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 51.35%\n",
            "\tTrain Loss: 0.676 | Train Acc: 55.57%\n",
            "\t Val. Loss: 0.692 |  Val. Acc: 53.53%\n",
            "\tTrain Loss: 0.533 | Train Acc: 65.58%\n",
            "\t Val. Loss: 0.715 |  Val. Acc: 54.85%\n",
            "\tTrain Loss: 0.481 | Train Acc: 67.84%\n",
            "\t Val. Loss: 0.895 |  Val. Acc: 53.18%\n"
          ]
        }
      ],
      "source": [
        "best_valid_loss = float('inf')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  train_epoch_loss = 0\n",
        "  train_epoch_acc = 0\n",
        "  for batch in train_iterator:\n",
        "      optimizer.zero_grad()\n",
        "      # retrieve text and no. of words\n",
        "      text, text_lengths = batch.review\n",
        "\n",
        "      #feedforward\n",
        "      # model.to(device)\n",
        "      predictions = model(text).squeeze(1)\n",
        "      \n",
        "      \n",
        "      loss = loss_func(predictions, batch.sentiment)\n",
        "\n",
        "      acc = accuracy(predictions, batch.sentiment)\n",
        "\n",
        "      # perform backpropagation\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      train_epoch_loss += loss.item()\n",
        "      train_epoch_acc += acc.item()\n",
        "\n",
        "  \n",
        "\n",
        "  valid_epoch_loss = 0\n",
        "  valid_epoch_acc = 0\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for batch in valid_iterator:\n",
        "          text, text_lengths = batch.review\n",
        "\n",
        "          predictions = model(text).squeeze(1)\n",
        "\n",
        "          loss = loss_func(predictions, batch.sentiment)\n",
        "\n",
        "          acc = accuracy(predictions, batch.sentiment)\n",
        "\n",
        "          valid_epoch_loss += loss.item()\n",
        "          valid_epoch_acc += acc.item()\n",
        "\n",
        "   \n",
        "\n",
        "  if valid_epoch_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_epoch_loss\n",
        "            torch.save(model.state_dict(), 'saved_weights'+'_linear.pt')\n",
        "\n",
        "  print(f'\\tTrain Loss: {train_epoch_loss / len(train_iterator):.3f} | Train Acc: {train_epoch_acc  / len(train_iterator)* 100:.2f}%')\n",
        "  print(f'\\t Val. Loss: {valid_epoch_loss / len(valid_iterator):.3f} |  Val. Acc: {valid_epoch_acc / len(valid_iterator) * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNHj9NZja4C-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}